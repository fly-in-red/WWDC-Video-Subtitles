
1
00:00:07,040 --> 00:00:16,416
(音楽)

2
00:00:21,688 --> 00:00:26,360
(拍手)

3
00:00:26,460 --> 00:00:28,228
おはようございます

4
00:00:29,863 --> 00:00:33,066
What's New in ARKit 2の
セッションへようこそ

5
00:00:34,701 --> 00:00:38,138
私はアルサラン
ARKitチームのエンジニアです

6
00:00:41,375 --> 00:00:46,446
iOS 11アップデートの一部として
ARKitを提供できて

7
00:00:47,247 --> 00:00:49,383
非常に胸が躍りました

8
00:00:51,051 --> 00:00:55,856
ARKitは無数のデバイスに
導入されています

9
00:00:55,956 --> 00:01:00,294
iOSは最大かつ最先端の
ARプラットフォームになりました

10
00:00:55,956 --> 00:01:00,294
iOSは最大かつ最先端の
ARプラットフォームになりました

11
00:01:03,163 --> 00:01:05,833
ARKitは簡単に操作可能な
インターフェイスを提供し

12
00:01:05,933 --> 00:01:07,868
優れた機能を備えてます

13
00:01:10,003 --> 00:01:15,242
皆さんのARKitでの製品には
本当に驚かされます

14
00:01:15,342 --> 00:01:18,312
App Storeから
いくつか紹介します

15
00:01:20,113 --> 00:01:22,816
ARアプリケーションの
Civilisations ARは

16
00:01:23,016 --> 00:01:25,986
歴史的遺物を
目の前に映し出します

17
00:01:26,153 --> 00:01:28,589
あらゆる角度から見られます

18
00:01:28,689 --> 00:01:34,194
Ｘ線モードにすれば
さらに色々なことができます

19
00:01:35,062 --> 00:01:38,232
裏庭に文化財を
持ってくることも

20
00:01:39,366 --> 00:01:44,071
数百年前の姿を
よみがえらせることも可能です

21
00:01:44,938 --> 00:01:48,475
歴史的遺物を見るには
絶好のツールです

22
00:01:50,577 --> 00:01:53,914
ARアプリケーションの
Boulevard ARでは

23
00:01:54,014 --> 00:01:58,585
芸術作品を今までにない形で
鑑賞できます

24
00:01:58,685 --> 00:02:01,221
地面に置いたり
壁にかけたり

25
00:01:58,685 --> 00:02:01,221
地面に置いたり
壁にかけたり

26
00:02:01,321 --> 00:02:03,724
近くに寄ったりもできます

27
00:02:03,824 --> 00:02:05,826
細部の鑑賞が可能です

28
00:02:05,959 --> 00:02:10,062
芸術について
学ぶには最適です

29
00:02:16,270 --> 00:02:19,706
ARKitを使えば
誰でも楽しく学習できます

30
00:02:20,040 --> 00:02:22,776
WWF Free Riversは
実物のような風景を

31
00:02:22,876 --> 00:02:25,078
目の前に映し出す
アプリケーションです

32
00:02:25,445 --> 00:02:29,049
風景を縫う川の流れを
たどることができ

33
00:02:29,149 --> 00:02:31,818
村や野生動物を見られます

34
00:02:32,019 --> 00:02:34,488
人間の活動により

35
00:02:34,888 --> 00:02:38,559
村や野生環境が
どんな影響を受けるのかを

36
00:02:38,659 --> 00:02:40,594
見ることができます

37
00:02:42,262 --> 00:02:46,867
環境保全や持続可能な
開発について学ぶには

38
00:02:46,967 --> 00:02:49,369
最高の教育ツールです

39
00:02:49,970 --> 00:02:55,275
他にも例は数多くあるので
App Storeを見てください

40
00:02:57,578 --> 00:03:00,113
ARKitを
ご存じない方のため

41
00:02:57,578 --> 00:03:00,113
ARKitを
ご存じない方のため

42
00:03:00,214 --> 00:03:04,117
ARKitが何なのか
手短にご説明します

43
00:03:08,388 --> 00:03:11,291
ARKitの主要コンポーネントは
トラッキングです

44
00:03:11,558 --> 00:03:15,796
現実世界でのデバイスの
位置と向きを算出します

45
00:03:18,899 --> 00:03:21,668
オブジェクトの
追跡もできます

46
00:03:21,768 --> 00:03:23,670
たとえば人間の顔

47
00:03:26,073 --> 00:03:27,808
シーンの理解により

48
00:03:28,375 --> 00:03:32,012
環境の特徴を学習し
トラッキングの精度を高めます

49
00:03:33,013 --> 00:03:34,448
検出できる対象は

50
00:03:34,982 --> 00:03:39,152
地面やテーブルの上などの
水平面です

51
00:03:39,586 --> 00:03:41,522
垂直面も検出できます

52
00:03:41,622 --> 00:03:45,292
これにより仮想オブジェクトを
シーンに出せます

53
00:03:50,998 --> 00:03:54,768
シーンの理解は
その環境下における

54
00:03:54,868 --> 00:03:57,104
照明の条件も学習します

55
00:03:57,938 --> 00:03:59,806
そのため 照明を用いて

56
00:03:59,907 --> 00:04:03,877
バーチャルなシーンに
実際の環境を正確に反映できます

57
00:03:59,907 --> 00:04:03,877
バーチャルなシーンに
実際の環境を正確に反映できます

58
00:04:03,977 --> 00:04:07,314
オブジェクトを
正しい明るさで表示します

59
00:04:09,917 --> 00:04:12,452
レンダリングはユーザが
デバイス上で目にするもので

60
00:04:12,553 --> 00:04:15,222
拡張現実のシーンと
連動します

61
00:04:16,223 --> 00:04:19,226
ARKitを使えば
とても手軽に

62
00:04:19,326 --> 00:04:22,729
好きなレンダリングエンジンと
統合できます

63
00:04:26,767 --> 00:04:31,205
ARKitはSceneKitとSpriteKitの
ビルトインビューを提供します

64
00:04:32,506 --> 00:04:35,809
Xcodeには
テンプレートがあり

65
00:04:35,909 --> 00:04:40,280
拡張現実体験を
すばやく作成できます

66
00:04:41,515 --> 00:04:46,453
UnityとUnreal Engineは
ARKitの全機能を統合した―

67
00:04:46,553 --> 00:04:49,156
人気のゲームエンジンです

68
00:04:49,256 --> 00:04:52,693
これらすべての
レンダリング技術が

69
00:04:52,793 --> 00:04:55,329
ARKitで利用できます

70
00:04:58,899 --> 00:05:02,069
今年のARKit 2の新機能を
見てみましょう

71
00:04:58,899 --> 00:05:02,069
今年のARKit 2の新機能を
見てみましょう

72
00:05:07,774 --> 00:05:10,043
まずはマップの保存とロード

73
00:05:10,143 --> 00:05:14,081
強力な新機能である
永続性を備え

74
00:05:14,181 --> 00:05:16,717
複数ユーザでの
体験も可能にします

75
00:05:19,653 --> 00:05:22,089
続いて
環境テクスチャリング

76
00:05:22,189 --> 00:05:25,993
拡張現実シーンの
リアルなレンダリングができます

77
00:05:27,628 --> 00:05:31,298
2D画像のリアルタイムでの
トラックが可能になりました

78
00:05:33,267 --> 00:05:37,871
2Dだけでなくシーン内の
3Dオブジェクトも検出できます

79
00:05:41,375 --> 00:05:45,245
そしてフェイストラッキングも
改良しました

80
00:05:48,081 --> 00:05:50,851
マップの保存とロードから
解説します

81
00:05:56,490 --> 00:06:00,194
マップの保存とロードは
ワールドトラッキングの一機能です

82
00:05:56,490 --> 00:06:00,194
マップの保存とロードは
ワールドトラッキングの一機能です

83
00:06:00,294 --> 00:06:04,231
ワールドトラッキングは
デバイスの位置と向きを定め

84
00:06:04,531 --> 00:06:07,935
現実世界の6DoFで
提示します

85
00:06:08,035 --> 00:06:10,604
これにより
オブジェクトをシーンに

86
00:06:11,138 --> 00:06:12,239
配置できます

87
00:06:12,506 --> 00:06:16,677
ご覧のテーブルとイスが
その例です

88
00:06:18,478 --> 00:06:21,381
ワールドトラッキングは
物理的スケールを正確に示し

89
00:06:21,481 --> 00:06:25,886
正しいスケールで
オブジェクトを配置できます

90
00:06:25,986 --> 00:06:29,389
不自然なサイズになることを
避けられます

91
00:06:33,927 --> 00:06:39,099
以前 紹介したMeasure
アプリケーションと同様に

92
00:06:39,199 --> 00:06:41,635
正確な計測が可能です

93
00:06:44,505 --> 00:06:47,107
ワールドトラッキングは
3D特徴点をとらえます

94
00:06:47,207 --> 00:06:51,845
環境の物理的な構造を
把握できます

95
00:06:51,945 --> 00:06:56,917
これを用いてシーンに
オブジェクトを配置できます

96
00:07:00,287 --> 00:07:04,391
リローカリゼーション機能は
iOS 11.3で導入されました

97
00:07:04,625 --> 00:07:09,296
この機能は
ARSessionの中断後

98
00:07:09,396 --> 00:07:11,798
トラッキングの状態を
復元します

99
00:07:12,733 --> 00:07:15,869
アプリケーションが
バックグラウンドで実行中の場合や

100
00:07:16,003 --> 00:07:19,406
iPadのピクチャ･イン･ピクチャを
使用中に作動します

101
00:07:23,944 --> 00:07:28,215
リローカリゼーションと
一緒に使えるのは

102
00:07:28,982 --> 00:07:32,686
ワールドトラッキングで
更新され続けるマップです

103
00:07:33,921 --> 00:07:36,924
環境を動き回れば
動き回るほど

104
00:07:37,024 --> 00:07:41,128
マップは拡大し
環境のさまざまな特徴を

105
00:07:41,228 --> 00:07:43,764
学習できるようになります

106
00:07:48,035 --> 00:07:51,471
このマップを
見ることができるのは

107
00:07:51,572 --> 00:07:54,174
ARSessionの
起動中だけでした

108
00:07:54,775 --> 00:07:58,345
このマップが
利用可能になりました

109
00:08:00,080 --> 00:08:01,815
ARKit APIにて

110
00:08:01,915 --> 00:08:05,919
ARWorldMapオブジェクトとして
マップを提供しています

111
00:08:14,127 --> 00:08:18,232
ARWorldMapは物理的な3D空間の
マッピングを表します

112
00:08:18,365 --> 00:08:23,871
右側のビジュアルにある図と
よく似たものです

113
00:08:25,739 --> 00:08:29,910
物理空間ではアンカーが
重要な地点となります

114
00:08:30,010 --> 00:08:34,214
仮想オブジェクトを
配置する場所です

115
00:08:34,381 --> 00:08:39,686
ARWorldMapでは通常のアンカーを
デフォルトで用意

116
00:08:40,854 --> 00:08:44,925
さらにカスタムアンカーを
リストに追加できます

117
00:08:45,025 --> 00:08:46,927
リストは変更可能です

118
00:08:47,027 --> 00:08:51,698
シーンにカスタムアンカーを作り
World Mapに追加できます

119
00:08:57,371 --> 00:09:00,240
視覚化とデバッグ用に

120
00:08:57,371 --> 00:09:00,240
視覚化とデバッグ用に

121
00:09:00,340 --> 00:09:03,877
原特徴点と範囲も提供しており

122
00:09:04,011 --> 00:09:09,383
スキャンした実際の
物理空間を把握できます

123
00:09:12,686 --> 00:09:16,690
さらに重要なことに
World Mapはシリアル化が可能

124
00:09:17,257 --> 00:09:21,028
どんなデータストリームにも
シリアル化できます

125
00:09:21,128 --> 00:09:23,330
ローカルシステム上の
ファイルや

126
00:09:23,430 --> 00:09:26,934
共有ネットワークの場所にも
シリアル化できます

127
00:09:29,536 --> 00:09:31,805
このARWorldMapの
オブジェクトは

128
00:09:31,905 --> 00:09:37,744
２種類の優れた体験を
ARKit上で可能にします

129
00:09:39,746 --> 00:09:42,149
１つ目は永続性です

130
00:09:44,151 --> 00:09:47,120
機能の実例を紹介しましょう

131
00:09:48,155 --> 00:09:51,792
ワールドトラッキングを
始めるユーザがいます

132
00:09:51,892 --> 00:09:54,361
彼はシーンに
オブジェクトを配置するため

133
00:09:54,461 --> 00:09:56,530
ARKitの
ヒットテストをします

134
00:09:57,531 --> 00:10:01,468
彼はその場を去る前に
World Mapを

135
00:09:57,531 --> 00:10:01,468
彼はその場を去る前に
World Mapを

136
00:10:01,702 --> 00:10:04,505
デバイスに保存します

137
00:10:08,876 --> 00:10:12,679
そして しばらく経ったあと

138
00:10:12,813 --> 00:10:14,515
ユーザが戻ってきます

139
00:10:15,015 --> 00:10:18,185
彼は同じWorld Mapを
ロードして

140
00:10:18,352 --> 00:10:22,389
同じ拡張現実を体験できます

141
00:10:22,523 --> 00:10:25,959
同じ体験を
何度でも繰り返せます

142
00:10:26,059 --> 00:10:31,298
毎回 テーブルの上に
同じオブジェクトが出てきます

143
00:10:31,398 --> 00:10:35,135
これがワールドトラッキングの
永続性です

144
00:10:35,802 --> 00:10:37,404
(拍手)

145
00:10:37,504 --> 00:10:39,072
ありがとうございます

146
00:10:39,173 --> 00:10:41,642
(拍手)

147
00:10:44,244 --> 00:10:48,015
ARWorldMapでは
複数ユーザの体験もできます

148
00:10:49,183 --> 00:10:54,188
単一のデバイスやユーザに
限定されず―

149
00:10:54,788 --> 00:10:57,858
拡張現実の体験は
多くのユーザと共有可能

150
00:11:00,160 --> 00:11:01,762
１人のユーザが

151
00:11:02,529 --> 00:11:07,167
World Mapを作成し
複数のユーザと共有します

152
00:11:08,302 --> 00:11:10,470
World Mapが表すのは

153
00:11:10,571 --> 00:11:14,474
現実世界の単一の座標系です

154
00:11:14,875 --> 00:11:19,880
つまりユーザ全員が
同じ座標空間を共有します

155
00:11:20,280 --> 00:11:25,853
同じ拡張現実を
違った角度から体験できます

156
00:11:27,487 --> 00:11:29,556
これは優れた新機能です

157
00:11:29,656 --> 00:11:31,258
World Mapを使って

158
00:11:31,859 --> 00:11:35,596
複数ユーザの
ゲームもできます

159
00:11:35,696 --> 00:11:37,898
昨日紹介したゲームなどです

160
00:11:39,700 --> 00:11:41,502
ARWorldMapを用いて

161
00:11:41,602 --> 00:11:46,306
複数ユーザでの共有体験を
教育にも生かせます

162
00:11:50,043 --> 00:11:53,647
ARWorldMap
オブジェクトを使って

163
00:11:53,747 --> 00:11:56,316
テクノロジーを自由に選び

164
00:11:57,518 --> 00:12:00,854
すべてのユーザと
共有できます

165
00:11:57,518 --> 00:12:00,854
すべてのユーザと
共有できます

166
00:12:02,289 --> 00:12:05,425
たとえば
共有手段にはAirDropや

167
00:12:05,526 --> 00:12:08,028
マルチピア接続が
利用できます

168
00:12:08,128 --> 00:12:11,064
Wi-FiやBluetoothによる
ローカルな通信です

169
00:12:11,932 --> 00:12:16,803
インターネット接続をせずに
この機能を使用できます

170
00:12:22,476 --> 00:12:26,180
ARKit APIが
World Mapの検索とロードに

171
00:12:26,280 --> 00:12:28,482
どう役立つか見てみます

172
00:12:30,918 --> 00:12:33,020
ARSessionのオブジェクトで

173
00:12:33,120 --> 00:12:36,156
getCurrentWorldMapを
呼び出します

174
00:12:36,256 --> 00:12:38,425
どの時点でも大丈夫です

175
00:12:39,693 --> 00:12:42,563
このメソッドには
完了ハンドラがあり

176
00:12:42,663 --> 00:12:46,400
ARWorldMapの
オブジェクトを返します

177
00:12:48,836 --> 00:12:53,941
worldMapが利用不可の場合
エラーを返します

178
00:12:54,041 --> 00:12:57,911
アプリケーションコード上で
このエラーの処理が必要です

179
00:12:59,613 --> 00:13:01,949
ARWorldMapを取得したら

180
00:12:59,613 --> 00:13:01,949
ARWorldMapを取得したら

181
00:13:02,850 --> 00:13:04,384
あとはシンプルです

182
00:13:07,254 --> 00:13:11,825
initialWorldMapプロパティを
設定し

183
00:13:12,025 --> 00:13:14,461
セッションを起動します

184
00:13:15,863 --> 00:13:18,632
動的に変更することも
可能です

185
00:13:18,732 --> 00:13:23,504
新しい構成で
ARSessionを再構成できます

186
00:13:25,472 --> 00:13:29,143
ARWorldMapとともに
ARSessionを起動したら

187
00:13:30,110 --> 00:13:32,880
iOS 11.3で導入した
リローカリゼーションと

188
00:13:32,980 --> 00:13:36,817
まったく同じように動きます

189
00:13:44,291 --> 00:13:48,128
ユーザ体験で重要なのは

190
00:13:48,228 --> 00:13:51,265
リローカリゼーションが
確実に機能することです

191
00:13:52,366 --> 00:13:56,136
優れたWorld Mapの入手が
必要です

192
00:13:56,236 --> 00:14:00,541
getCurrentWorldMapは
いつでも呼び出せます

193
00:13:56,236 --> 00:14:00,541
getCurrentWorldMapは
いつでも呼び出せます

194
00:14:02,376 --> 00:14:06,446
肝心なのは
物理空間を複数の視点から

195
00:14:06,680 --> 00:14:08,315
スキャンすることです

196
00:14:08,415 --> 00:14:13,820
トラッキングシステムに環境の
物理的な構造を学習させます

197
00:14:16,190 --> 00:14:19,126
環境は静的で
きめ細かいほうが

198
00:14:19,226 --> 00:14:22,329
より多くの特徴が抽出可能で

199
00:14:22,429 --> 00:14:24,765
環境をより細かく把握できます

200
00:14:27,768 --> 00:14:31,972
マップ上に特徴点を
密集させることも重要です

201
00:14:32,072 --> 00:14:35,509
確実なリローカライズに
必要です

202
00:14:37,244 --> 00:14:40,848
しかし これらの点について
心配は無用です

203
00:14:41,515 --> 00:14:45,419
ARKit APIでは
操作を簡単にするため

204
00:14:45,519 --> 00:14:48,422
WorldMappingStatusを
ARFrameで提供してます

205
00:14:49,223 --> 00:14:52,960
WorldMappingStatusは
すべてのARFrameで更新され

206
00:14:53,060 --> 00:14:56,830
WorldMappingStatus
プロパティで取得できます

207
00:14:56,964 --> 00:14:58,999
手順を見てみましょう

208
00:15:01,468 --> 00:15:04,805
ワールドトラッキングの
開始時での表示はNot Available

209
00:15:04,905 --> 00:15:09,209
物理空間のスキャン開始で
Limitedになります

210
00:15:10,511 --> 00:15:13,547
物理空間を動き回るにつれて

211
00:15:13,981 --> 00:15:17,084
マップが拡張されて
Extendingとなります

212
00:15:19,386 --> 00:15:22,222
現在の視点で
十分にスキャンしたら

213
00:15:22,322 --> 00:15:26,593
WorldMappingStatusが
Mappedになります

214
00:15:34,434 --> 00:15:39,506
マップにある物理空間から
別の方向へ向けると

215
00:15:39,773 --> 00:15:42,876
WorldMappingStatusが
Limitedに戻ります

216
00:15:42,976 --> 00:15:48,749
見えている新しい環境を
学習し始めます

217
00:15:51,151 --> 00:15:54,555
アプリケーションコード上での
WorldMappingStatusの使用法

218
00:15:56,390 --> 00:16:01,361
World Mapを他のユーザと共有する
アプリケーションがあるとします

219
00:15:56,390 --> 00:16:01,361
World Mapを他のユーザと共有する
アプリケーションがあるとします

220
00:16:01,695 --> 00:16:05,766
インターフェイスのボタンで
マップの共有が可能

221
00:16:07,301 --> 00:16:11,572
WorldMappingStatusが
notAvailableまたはlimitedの時は

222
00:16:11,672 --> 00:16:12,940
ボタンを無効にします

223
00:16:14,441 --> 00:16:17,311
WorldMappingStatusが
extendingの時は

224
00:16:18,178 --> 00:16:21,715
アクティビティインジケータを
UIに表示します

225
00:16:21,815 --> 00:16:24,184
これによりエンドユーザに

226
00:16:24,284 --> 00:16:30,224
物理的な世界の移動と
スキャンとマップの拡張を促します

227
00:16:30,390 --> 00:16:32,559
リローカリゼーションに
必要なことです

228
00:16:36,430 --> 00:16:39,500
WorldMappingStatusが
mappedになったら

229
00:16:40,167 --> 00:16:45,472
Share Mapボタンを有効にして
アクティビティインジケータを隠す

230
00:16:45,906 --> 00:16:48,942
これでユーザは
マップの共有ができます

231
00:16:53,547 --> 00:16:56,984
マップの保存とロードを
実演します

232
00:16:58,552 --> 00:17:04,424
(拍手)

233
00:16:58,552 --> 00:17:04,424
(拍手)

234
00:17:06,560 --> 00:17:07,627
オーケイ

235
00:17:09,396 --> 00:17:11,498
AR1に切り替えます

236
00:17:13,166 --> 00:17:13,967
オーケイ

237
00:17:14,535 --> 00:17:17,438
２つのアプリケーションを
見ていきます

238
00:17:17,538 --> 00:17:19,239
そのうち１つ目では

239
00:17:19,339 --> 00:17:23,010
World Mapを取得し
ローカルファイルに保存

240
00:17:23,210 --> 00:17:27,614
２つ目では
同じWorld Mapをロードし

241
00:17:27,981 --> 00:17:31,285
同じ拡張現実の体験を
復元します

242
00:17:31,385 --> 00:17:32,719
始めましょう

243
00:17:35,956 --> 00:17:39,626
WorldMappingStatusが
右上の隅に示されます

244
00:17:39,726 --> 00:17:41,495
Not Availableです

245
00:17:41,662 --> 00:17:44,832
環境内を動き始めると

246
00:17:44,965 --> 00:17:48,202
World Mapが拡張されます

247
00:17:48,302 --> 00:17:52,339
この環境のマッピングと
移動を続けると

248
00:17:53,240 --> 00:17:56,476
WorldMappingStatusが
Mappedに変わります

249
00:17:57,044 --> 00:18:00,414
この視点からは
十分に特徴を把握したので

250
00:17:57,044 --> 00:18:00,414
この視点からは
十分に特徴を把握したので

251
00:18:00,514 --> 00:18:03,517
リローカリゼーションが
可能ということです

252
00:18:03,617 --> 00:18:08,722
World Mapのオブジェクトを取得し
シリアル化していきます

253
00:18:10,124 --> 00:18:12,526
その前に
カスタムアンカーを配置して

254
00:18:12,626 --> 00:18:16,597
拡張現実のシーンを
もっと面白くしましょう

255
00:18:16,964 --> 00:18:18,899
ヒットテストにより

256
00:18:19,099 --> 00:18:21,101
カスタムアンカーを
作りました

257
00:18:21,201 --> 00:18:22,936
このオブジェクトを

258
00:18:23,737 --> 00:18:25,038
オーバーレイします

259
00:18:25,139 --> 00:18:27,674
旧式のテレビです

260
00:18:27,808 --> 00:18:30,510
昔 見たことがあるでしょう

261
00:18:33,347 --> 00:18:37,451
もちろん
マッピングも続けられます

262
00:18:37,718 --> 00:18:40,721
World Mapを保存しましょう

263
00:18:41,255 --> 00:18:43,724
World Mapを保存した際

264
00:18:43,957 --> 00:18:47,561
このWorld Mapに属する
特徴点を表示できます

265
00:18:47,928 --> 00:18:52,666
青い点はすべて
私のWorld Mapの一部です

266
00:18:55,402 --> 00:18:57,271
よい方法として―

267
00:18:58,605 --> 00:19:01,275
World Map保存時の視点を

268
00:18:58,605 --> 00:19:01,275
World Map保存時の視点を

269
00:19:01,642 --> 00:19:04,211
スクリーンショットで残します

270
00:19:07,247 --> 00:19:11,718
World Mapをファイルに
シリアル化しました

271
00:19:12,052 --> 00:19:16,457
別のアプリケーションで
同じ拡張現実を復元します

272
00:19:16,924 --> 00:19:18,559
やってみましょう

273
00:19:18,792 --> 00:19:21,895
アプリケーションを
違う位置で起動します

274
00:19:24,231 --> 00:19:26,567
これが私の
ワールドの起点です

275
00:19:26,667 --> 00:19:29,303
テーブルのこちら側で
定義されてます

276
00:19:29,403 --> 00:19:32,873
ワールドトラッキングは
リローカライズ状態です

277
00:19:33,040 --> 00:19:37,911
開始時の動作はiOS 11.3で
導入したものと同じです

278
00:19:38,445 --> 00:19:41,815
今度はデバイスの方向を

279
00:19:42,082 --> 00:19:47,354
World Mapを作成した
物理空間に向けてみます

280
00:19:48,589 --> 00:19:51,658
同じ空間に向いたとたん

281
00:19:51,758 --> 00:19:56,230
ワールドの起点が
元の位置に復元されます

282
00:19:56,396 --> 00:20:00,701
同時にカスタムアンカーも
復元されました

283
00:19:56,396 --> 00:20:00,701
同時にカスタムアンカーも
復元されました

284
00:20:00,801 --> 00:20:03,403
完全に同じAR体験です

285
00:20:03,504 --> 00:20:09,142
(拍手)

286
00:20:09,243 --> 00:20:10,310
ありがとうございます

287
00:20:11,411 --> 00:20:15,516
このアプリケーションは
何度でも起動でき

288
00:20:15,616 --> 00:20:19,286
起動時には毎回
同じ体験を提供します

289
00:20:19,386 --> 00:20:21,255
これが永続性です

290
00:20:22,289 --> 00:20:25,726
当然 他のデバイスと
共有もできます

291
00:20:27,528 --> 00:20:29,229
スライドに戻ります

292
00:20:34,401 --> 00:20:37,504
以上がマップの
保存とロードでした

293
00:20:37,871 --> 00:20:40,574
ARKit 2の優れた新機能で

294
00:20:40,674 --> 00:20:43,277
永続性を可能にし

295
00:20:43,377 --> 00:20:45,879
複数ユーザで
体験を共有できます

296
00:20:49,583 --> 00:20:54,288
ARKit 2では より速い
初期化と平面検出を実現

297
00:20:56,723 --> 00:20:59,026
ワールドトラッキングは
より強力になり

298
00:20:59,126 --> 00:21:02,729
難しい環境でも
平面を検出できます

299
00:20:59,126 --> 00:21:02,729
難しい環境でも
平面を検出できます

300
00:21:07,568 --> 00:21:12,439
水平面と垂直面の範囲と境界が
より正確になりました

301
00:21:12,539 --> 00:21:16,310
オブジェクトをシーンに
正確に配置できます

302
00:21:20,114 --> 00:21:22,849
iOS 11.3で導入された
機能の１つ

303
00:21:22,950 --> 00:21:27,287
拡張現実体験での
連続オートフォーカス

304
00:21:28,489 --> 00:21:32,426
iOS 12では
拡張現実体験に特化し

305
00:21:32,526 --> 00:21:35,562
さらに最適化されています

306
00:21:38,999 --> 00:21:44,505
4:3のビデオフォーマットを
ARKitに導入します

307
00:21:47,141 --> 00:21:50,043
4:3は広角ビデオフォーマットで

308
00:21:50,144 --> 00:21:54,481
iPadの画面アスペクト比も
4:3なので

309
00:21:54,581 --> 00:21:58,285
iPad上での視覚化を
大きく向上させます

310
00:21:59,653 --> 00:22:04,792
ARKit 2のビデオフォーマットは
4:3がデフォルトになります

311
00:21:59,653 --> 00:22:04,792
ARKit 2のビデオフォーマットは
4:3がデフォルトになります

312
00:22:06,660 --> 00:22:08,729
すべての改良点は

313
00:22:08,829 --> 00:22:12,933
App Storeにある
全アプリケーションに反映されます

314
00:22:13,033 --> 00:22:15,302
4:3のビデオフォーマットは
例外で

315
00:22:15,402 --> 00:22:19,873
新規のSTKでアプリケーションを
構築する必要があります

316
00:22:23,243 --> 00:22:25,045
話を戻しましょう

317
00:22:25,779 --> 00:22:28,715
エンドユーザの
体験の向上です

318
00:22:30,751 --> 00:22:33,220
環境テクスチャリングを
導入します

319
00:22:34,021 --> 00:22:37,391
これでレンダリングが
大幅に改善され

320
00:22:37,491 --> 00:22:39,326
エンドユーザの体験が
向上します

321
00:22:41,161 --> 00:22:43,764
デザイナーが懸命に努力して

322
00:22:43,864 --> 00:22:47,601
これらの仮想オブジェクトを
作ったとします

323
00:22:47,701 --> 00:22:49,603
拡張現実用のものです

324
00:22:50,804 --> 00:22:52,539
よくできてますが

325
00:22:52,673 --> 00:22:54,675
拡張現実に使うには

326
00:22:54,942 --> 00:22:57,744
もうひと工夫 必要です

327
00:22:59,546 --> 00:23:02,049
やるべきことは

328
00:22:59,546 --> 00:23:02,049
やるべきことは

329
00:23:02,149 --> 00:23:06,420
ARのシーン上で
位置と向きを正しくすることです

330
00:23:06,520 --> 00:23:11,558
実際に 現実世界に
オブジェクトがあるように見せます

331
00:23:13,560 --> 00:23:15,929
正しい大きさも重要です

332
00:23:16,029 --> 00:23:18,899
大きすぎも
小さすぎもダメです

333
00:23:18,999 --> 00:23:22,970
ARKitは
ワールドトラッキングにおいて

334
00:23:23,070 --> 00:23:25,038
正しい変換をしてくれます

335
00:23:28,642 --> 00:23:31,945
リアルなレンダリングに
大切なのは

336
00:23:32,045 --> 00:23:33,947
環境の照明です

337
00:23:37,151 --> 00:23:41,555
ARKitでは環境光推定を
レンダリングに使い

338
00:23:41,755 --> 00:23:45,058
オブジェクトの
明るさを修正します

339
00:23:45,192 --> 00:23:49,096
明るすぎたり
暗すぎたりすることなく

340
00:23:49,229 --> 00:23:51,565
環境になじみます

341
00:23:54,835 --> 00:24:00,174
オブジェクトを水平面など
物理的な表面に配置する場合

342
00:23:54,835 --> 00:24:00,174
オブジェクトを水平面など
物理的な表面に配置する場合

343
00:24:00,407 --> 00:24:04,211
オブジェクトに
影をつけることも大切です

344
00:24:04,311 --> 00:24:07,781
影は人の視覚に
大きく影響します

345
00:24:07,881 --> 00:24:11,018
オブジェクトが本当に
表面上にあると認識されます

346
00:24:13,253 --> 00:24:14,922
最後の項目は

347
00:24:16,023 --> 00:24:17,858
反射するオブジェクトです

348
00:24:18,692 --> 00:24:20,561
人間の目は―

349
00:24:20,928 --> 00:24:24,832
仮想オブジェクトの表面に
環境の反射を期待します

350
00:24:25,632 --> 00:24:29,469
環境テクスチャリングにより
これが可能になります

351
00:24:31,405 --> 00:24:35,976
拡張現実のシーンで
どう見えるか確認しましょう

352
00:24:38,045 --> 00:24:43,116
昨晩 本セッションの準備中に
このシーンを作りました

353
00:24:44,618 --> 00:24:50,157
果物を食べながら
仮想オブジェクトを配置したのです

354
00:24:51,692 --> 00:24:55,762
ご覧のとおり
大きさは正確です

355
00:24:55,996 --> 00:24:58,432
さらに重要なことに

356
00:24:58,532 --> 00:25:01,368
環境がオブジェクトに
反射してます

357
00:24:58,532 --> 00:25:01,368
環境がオブジェクトに
反射してます

358
00:25:02,336 --> 00:25:05,038
オブジェクトの右側には

359
00:25:05,139 --> 00:25:10,577
黄色とオレンジ色の
果物の反射が映っています

360
00:25:10,844 --> 00:25:15,182
左側には
葉の緑色が見えます

361
00:25:15,949 --> 00:25:20,053
中央にはベンチの表面が
反射しています

362
00:25:21,422 --> 00:25:25,792
ARKit 2の環境テクスチャリングで
これが可能になりました

363
00:25:27,661 --> 00:25:29,129
(拍手)

364
00:25:29,229 --> 00:25:30,097
ありがとう

365
00:25:34,167 --> 00:25:37,871
シーンのテクスチャ情報を
環境テクスチャリングが収集

366
00:25:40,774 --> 00:25:46,346
通常 キューブマップで表され
他の形式もあります

367
00:25:49,049 --> 00:25:51,985
環境テクスチャ
またはキューブマップは

368
00:25:52,085 --> 00:25:55,956
レンダリングエンジンで
反射プローブとして使えます

369
00:25:58,826 --> 00:26:04,565
テクスチャ情報は
仮想オブジェクトに適用されます

370
00:25:58,826 --> 00:26:04,565
テクスチャ情報は
仮想オブジェクトに適用されます

371
00:26:04,665 --> 00:26:06,867
前のスライドが その例です

372
00:26:07,668 --> 00:26:09,770
反射オブジェクトの視覚化が

373
00:26:11,505 --> 00:26:14,041
大幅に向上しました

374
00:26:14,908 --> 00:26:19,847
それでは この仕組みを
ビデオでお見せします

375
00:26:22,249 --> 00:26:26,687
ARKitはワールドトラッキングと
シーン理解を作動させ

376
00:26:26,787 --> 00:26:29,456
環境の学習を続けます

377
00:26:30,090 --> 00:26:33,126
コンピュータビジョンを
用いて

378
00:26:33,393 --> 00:26:38,031
テクスチャ情報を抽出し
キューブマップを埋めます

379
00:26:39,500 --> 00:26:42,870
キューブマップはシーンに
正確に配置されてます

380
00:26:44,671 --> 00:26:47,741
キューブマップは
一部だけ埋まってます

381
00:26:48,976 --> 00:26:54,314
反射プローブの設定には
完全に埋まったマップが必要

382
00:26:57,451 --> 00:26:59,186
キューブマップを
完全に埋めるには

383
00:26:59,286 --> 00:27:03,323
物理空間全体を
スキャンする必要があります

384
00:26:59,286 --> 00:27:03,323
物理空間全体を
スキャンする必要があります

385
00:27:03,423 --> 00:27:05,626
パノラマと同様の

386
00:27:05,726 --> 00:27:07,594
360度スキャンです

387
00:27:08,428 --> 00:27:12,065
それはエンドユーザにとって
実用的ではありません

388
00:27:13,467 --> 00:27:16,303
ARKitは作業を
簡単にするため

389
00:27:16,870 --> 00:27:19,540
機械学習の
アルゴリズムを使い

390
00:27:19,640 --> 00:27:22,476
キューブマップを
自動で埋めます

391
00:27:23,277 --> 00:27:29,383
(拍手)

392
00:27:31,385 --> 00:27:37,224
処理はすべてデバイス上で
リアルタイムに行われます

393
00:27:39,927 --> 00:27:43,564
キューブマップが完成したら
反射プローブを設定します

394
00:27:43,764 --> 00:27:47,100
仮想オブジェクトを
シーンに配置すると同時に

395
00:27:47,234 --> 00:27:49,970
環境の反射が開始します

396
00:27:51,104 --> 00:27:55,409
以上が環境テクスチャリングの
概要です

397
00:27:57,511 --> 00:28:03,217
ARKit APIはどのように
この機能を有効にしていくか

398
00:27:57,511 --> 00:28:03,217
ARKit APIはどのように
この機能を有効にしていくか

399
00:28:07,688 --> 00:28:12,092
ワールドトラッキングの
構成でやるべきことは

400
00:28:12,259 --> 00:28:15,996
environmentTexturing
プロパティをautomaticにして

401
00:28:17,030 --> 00:28:18,665
セッションを起動

402
00:28:19,032 --> 00:28:21,034
こんなに簡単です

403
00:28:21,835 --> 00:28:27,841
(拍手)

404
00:28:29,543 --> 00:28:34,548
ARSessionはバックグラウンドで
環境テクスチャリングを行い

405
00:28:34,648 --> 00:28:37,484
環境プローブアンカーとして

406
00:28:37,885 --> 00:28:40,020
環境テクスチャを提供します

407
00:28:41,155 --> 00:28:44,725
AREnvironmentProbeAnchorは
ARAnchorのExtensionです

408
00:28:44,825 --> 00:28:49,429
6DoF･位置･向きを示す
transformを持ちます

409
00:28:50,898 --> 00:28:55,002
さらにMTLTextureの形式の
キューブマップがあります

410
00:28:58,272 --> 00:29:02,543
ARKitはキューブマップの
物理的な範囲も示します

411
00:28:58,272 --> 00:29:02,543
ARKitはキューブマップの
物理的な範囲も示します

412
00:29:02,643 --> 00:29:05,078
これが反射プローブの

413
00:29:05,179 --> 00:29:07,648
影響が及ぶ範囲です

414
00:29:07,781 --> 00:29:12,486
レンダリングの主体により
平行の修正に用いられます

415
00:29:12,619 --> 00:29:15,455
たとえば
オブジェクトが移動する場合

416
00:29:15,556 --> 00:29:18,058
自動で新しい位置に対応し

417
00:29:18,158 --> 00:29:22,529
新たなテクスチャが
環境に反映されます

418
00:29:24,665 --> 00:29:28,702
他のアンカーと同じ
ライフサイクルをたどります

419
00:29:28,802 --> 00:29:32,105
ARPlaneAnchorや
ARImageAnchorと同様です

420
00:29:36,777 --> 00:29:40,714
さらにARSCNViewに
統合されています

421
00:29:40,814 --> 00:29:45,252
レンダリングに
SceneKitを使用する場合

422
00:29:45,619 --> 00:29:50,791
ワールドトラッキング構成で
この機能を有効にします

423
00:29:50,891 --> 00:29:54,495
あとはARSCNViewが
自動的に処理します

424
00:30:00,367 --> 00:30:02,870
高度な使い方として

425
00:30:02,970 --> 00:30:07,641
環境プローブアンカーを
手動でシーンに配置できます

426
00:30:11,445 --> 00:30:15,949
これにはenvironmentTexturingを
manualにします

427
00:30:16,417 --> 00:30:17,518
そのあとは

428
00:30:18,151 --> 00:30:22,089
環境プローブアンカーを
好きな位置や向きに作成し

429
00:30:22,189 --> 00:30:25,993
ARSessionの
オブジェクトに追加します

430
00:30:28,862 --> 00:30:33,300
この方法でシーンに
プローブアンカーを配置できます

431
00:30:33,667 --> 00:30:38,272
環境のさらなる情報を得ると
ARSessionは自動的に

432
00:30:38,372 --> 00:30:40,407
テクスチャを更新します

433
00:30:42,176 --> 00:30:45,479
手動モードが使えるのは

434
00:30:45,579 --> 00:30:49,016
拡張現実のオブジェクトが
１つの場合です

435
00:30:49,116 --> 00:30:53,854
大量の環境プローブアンカーで
システムに負担をかけずに済みます

436
00:30:57,090 --> 00:31:01,161
環境テクスチャリングの
実演を行い

437
00:30:57,090 --> 00:31:01,161
環境テクスチャリングの
実演を行い

438
00:31:01,261 --> 00:31:05,299
拡張現実をリアルに
レンダリングしてお見せします

439
00:31:06,033 --> 00:31:11,004
(拍手)

440
00:31:16,176 --> 00:31:19,113
AR1に切り替えます

441
00:31:23,517 --> 00:31:25,719
オーケイ　この実演では

442
00:31:26,086 --> 00:31:28,856
ワールドトラッキング構成を
作動させます

443
00:31:28,956 --> 00:31:32,459
環境テクスチャリング機能は
無効にします

444
00:31:33,393 --> 00:31:34,394
それでは

445
00:31:35,062 --> 00:31:38,565
画面下に
スイッチコントローラがあります

446
00:31:38,665 --> 00:31:41,468
環境光の推定を用いています

447
00:31:41,635 --> 00:31:46,206
前に見たオブジェクトを
配置しましょう

448
00:31:46,940 --> 00:31:49,309
ご覧のとおり

449
00:31:50,477 --> 00:31:52,479
問題はないようです

450
00:31:52,579 --> 00:31:55,582
テーブルに載っていて
影もあります

451
00:31:55,916 --> 00:31:58,685
上出来のARシーンです

452
00:31:59,520 --> 00:32:02,890
足りないのは
テーブルの木の表面が

453
00:31:59,520 --> 00:32:02,890
足りないのは
テーブルの木の表面が

454
00:32:02,990 --> 00:32:05,058
反映されていないことです

455
00:32:06,693 --> 00:32:10,063
このシーンに さらに何かを
置いてみましょう

456
00:32:10,964 --> 00:32:12,733
たとえば本物の果物

457
00:32:16,036 --> 00:32:21,108
仮想オブジェクトには
それが映りません

458
00:32:21,808 --> 00:32:25,579
環境テクスチャリングを
有効にして このテクスチャが

459
00:32:27,014 --> 00:32:29,850
どうリアルに
表されるか試します

460
00:32:31,118 --> 00:32:34,154
環境テクスチャリングを
有効にすると同時に

461
00:32:34,388 --> 00:32:38,759
オブジェクトで
木の表面が反射されます

462
00:32:38,859 --> 00:32:42,663
バナナのテクスチャも
映っています

463
00:32:42,896 --> 00:32:48,902
(拍手)

464
00:32:50,671 --> 00:32:51,438
ありがとう

465
00:32:52,439 --> 00:32:57,010
これは拡張現実のシーンを
大幅に改善します

466
00:32:57,110 --> 00:33:00,013
可能な限り
リアルになります

467
00:32:57,110 --> 00:33:00,013
可能な限り
リアルになります

468
00:33:00,614 --> 00:33:02,816
本当にテーブル上に
あるようです

469
00:33:04,418 --> 00:33:06,620
スライドに戻します

470
00:33:12,593 --> 00:33:14,628
以上が
環境テクスチャリングです

471
00:33:14,728 --> 00:33:18,065
ARKit 2の
優れた新機能であり

472
00:33:18,165 --> 00:33:24,171
可能な限り リアルな
拡張現実のシーンを作り出します

473
00:33:25,405 --> 00:33:29,877
続いて すばらしい新機能を
説明するのは

474
00:33:29,977 --> 00:33:32,780
これから登壇する
ラインハルトです

475
00:33:33,146 --> 00:33:38,919
(拍手)

476
00:33:42,222 --> 00:33:42,956
入ってる？

477
00:33:43,056 --> 00:33:44,258
オーケイ　よし

478
00:33:45,659 --> 00:33:46,427
おはようございます

479
00:33:47,227 --> 00:33:48,395
私はラインハルト

480
00:33:48,495 --> 00:33:50,464
ARKitチームの
エンジニアです

481
00:33:50,564 --> 00:33:53,300
次はイメージトラッキングの
話をします

482
00:33:54,034 --> 00:33:55,469
iOS 11.3では

483
00:33:55,569 --> 00:33:58,906
ワールドトラッキングに
イメージ検出を導入しました

484
00:33:59,573 --> 00:34:03,477
イメージ検出は
シーン上の2Dイメージを探します

485
00:33:59,573 --> 00:34:03,477
イメージ検出は
シーン上の2Dイメージを探します

486
00:34:04,311 --> 00:34:07,614
検出されるイメージは
静止画です

487
00:34:07,714 --> 00:34:09,917
動きのないものです

488
00:34:10,184 --> 00:34:15,188
映画のポスターや
美術館の絵画がその好例です

489
00:34:16,255 --> 00:34:20,194
ARKitは
イメージが検出されると

490
00:34:20,293 --> 00:34:23,864
6DoFで位置と向きを
推定します

491
00:34:24,364 --> 00:34:28,402
これを用いて
シーンにコンテンツを表示します

492
00:34:29,469 --> 00:34:33,774
この機能はワールドトラッキングに
統合されています

493
00:34:33,873 --> 00:34:37,478
プロパティで
１回 設定するだけです

494
00:34:38,879 --> 00:34:42,014
イメージ検出に用いる
イメージをロードするには

495
00:34:42,114 --> 00:34:45,786
ファイルからのロードか
Xcodeのアセットカタログを使用

496
00:34:45,886 --> 00:34:49,456
これでイメージを検出する
クオリティが高まります

497
00:34:50,224 --> 00:34:52,125
イメージ検出は有能です

498
00:34:52,226 --> 00:34:54,695
iOS 12では
さらに改良しました

499
00:34:54,795 --> 00:34:57,364
イメージトラッキングについて
話しましょう

500
00:34:57,664 --> 00:35:00,768
イメージトラッキングは
イメージ検出のExtensionです

501
00:34:57,664 --> 00:35:00,768
イメージトラッキングは
イメージ検出のExtensionです

502
00:35:00,868 --> 00:35:06,006
長所はイメージが
静止画でなくてもいい点です

503
00:35:07,474 --> 00:35:11,411
ARKitが位置と向きを
フレームごとに推定します

504
00:35:11,511 --> 00:35:13,213
１秒に60フレームです

505
00:35:13,547 --> 00:35:16,483
これにより2Dイメージを

506
00:35:16,650 --> 00:35:18,485
正確に取り込めます

507
00:35:18,585 --> 00:35:23,824
雑誌やボードゲーム
実像があれば何でも取り込めます

508
00:35:25,025 --> 00:35:29,463
ARKitは複数のイメージを
同時に追跡できます

509
00:35:30,731 --> 00:35:32,866
デフォルトでは
イメージを１つだけ選択

510
00:35:32,966 --> 00:35:35,669
雑誌の表紙などの場合は

511
00:35:35,769 --> 00:35:38,739
設定は１を推奨します

512
00:35:38,839 --> 00:35:42,609
雑誌の見開きページの場合は

513
00:35:42,709 --> 00:35:44,144
２に設定します

514
00:35:45,245 --> 00:35:49,483
そしてiOS 12の
ARKit 2には

515
00:35:49,583 --> 00:35:53,387
ARImageTrackingConfiguration
という最新の構成があり

516
00:35:53,487 --> 00:35:56,957
単独でのイメージトラッキングを
可能にします

517
00:35:57,191 --> 00:35:58,992
設定方法を説明します

518
00:35:59,860 --> 00:36:02,062
まず 参照するイメージを

519
00:35:59,860 --> 00:36:02,062
まず 参照するイメージを

520
00:36:02,162 --> 00:36:04,865
ファイルか
アセットカタログからロード

521
00:36:05,599 --> 00:36:08,435
参照するイメージを
ロードし終えたら

522
00:36:08,569 --> 00:36:12,306
次の２種類の
セッションの設定に用います

523
00:36:12,406 --> 00:36:16,176
ARWorldTrackingConfigurationに
detectionImagesを指定するか

524
00:36:16,276 --> 00:36:21,281
ARImageTrackingConfigurationに
trackingImagesを指定します

525
00:36:22,249 --> 00:36:27,254
設定が完了したら
セッションを起動します

526
00:36:28,222 --> 00:36:29,456
いつものとおり―

527
00:36:29,823 --> 00:36:33,894
セッションが起動すると
更新ごとにARFrameを取得

528
00:36:34,328 --> 00:36:38,832
ARFrameにはARImageAnchorの
オブジェクトが含まれます

529
00:36:38,932 --> 00:36:41,034
イメージ検出後のことです

530
00:36:42,369 --> 00:36:45,138
このARImageAnchorは
追跡可能です

531
00:36:45,239 --> 00:36:48,509
ARTrackableのプロトコルで
確認できます

532
00:36:48,675 --> 00:36:50,577
ここにはBooleanの

533
00:36:51,445 --> 00:36:52,913
isTrackedがあります

534
00:36:53,013 --> 00:36:56,717
イメージの
トラッキング状態を示します

535
00:36:56,817 --> 00:36:59,753
trueだと追跡されています

536
00:37:00,354 --> 00:37:04,425
どのイメージが どこで
検出されたかも分かり

537
00:37:04,525 --> 00:37:09,730
イメージの位置と向きが
４行４列のマトリクスで示されます

538
00:37:11,665 --> 00:37:15,769
イメージアンカーを得るため
まずはイメージをロード

539
00:37:15,869 --> 00:37:19,439
よいイメージとは何か
見てみましょう

540
00:37:19,707 --> 00:37:23,043
これは児童書にあるイメージです

541
00:37:23,143 --> 00:37:26,413
イメージトラッキングに
適しています

542
00:37:26,680 --> 00:37:28,515
視覚的特徴が顕著で

543
00:37:28,615 --> 00:37:31,618
細かく描かれ
コントラストが明確です

544
00:37:32,552 --> 00:37:34,655
一方 こちらのイメージは

545
00:37:34,755 --> 00:37:38,225
同じく子どもの
教科書にありますが

546
00:37:38,692 --> 00:37:40,394
お薦めできません

547
00:37:40,594 --> 00:37:43,897
反復が多く 色領域は単調

548
00:37:43,997 --> 00:37:47,334
グレースケールに変換すると
ヒストグラムが狭いです

549
00:37:48,302 --> 00:37:52,439
こういうデータを調べなくても
Xcodeが役に立ちます

550
00:37:52,773 --> 00:37:55,075
２つのイメージをXcodeに
インポートすると

551
00:37:55,943 --> 00:38:00,280
推奨される海のイメージには
警告が表示されず

552
00:37:55,943 --> 00:38:00,280
推奨される海のイメージには
警告が表示されず

553
00:38:00,514 --> 00:38:05,152
子ども３人のイメージには
警告アイコンが出ます

554
00:38:05,919 --> 00:38:09,089
アイコンをクリックすると
詳細が出て

555
00:38:09,189 --> 00:38:12,059
なぜ このイメージが
イメージトラッキングに

556
00:38:12,159 --> 00:38:13,994
向かないか分かります

557
00:38:14,161 --> 00:38:17,798
ヒストグラムや
単調な色領域について

558
00:38:17,898 --> 00:38:19,767
情報が出てきます

559
00:38:21,335 --> 00:38:26,507
イメージのロードが終わると
２種類の構成から選べます

560
00:38:26,607 --> 00:38:29,343
１つは
ARWorldTrackingConfiguration

561
00:38:29,443 --> 00:38:30,778
これについて解説します

562
00:38:33,213 --> 00:38:35,749
ワールドトラッキングで
イメージトラッキングを使うと

563
00:38:35,849 --> 00:38:39,086
イメージアンカーは
ワールド座標系上に表されます

564
00:38:39,186 --> 00:38:43,457
イメージアンカー
またはオプションで平面アンカー

565
00:38:44,224 --> 00:38:49,596
そしてカメラやワールドの起点も
すべて同じ座標に出ます

566
00:38:49,697 --> 00:38:52,933
それぞれのインタラクションは
これで簡単に

567
00:38:54,001 --> 00:38:56,236
iOS 12の新機能は

568
00:38:56,336 --> 00:39:00,741
イメージの検出だけでなく
追跡もできます

569
00:38:56,336 --> 00:39:00,741
イメージの検出だけでなく
追跡もできます

570
00:39:02,209 --> 00:39:05,546
ARImageTrackingConfigurationは
新しい構成です

571
00:39:05,646 --> 00:39:08,315
単独でイメージトラッキングを
行います

572
00:39:08,916 --> 00:39:11,251
ワールドトラッキングから
独立し

573
00:39:11,351 --> 00:39:15,823
トラッキング時に
モーションセンサーを用いません

574
00:39:16,056 --> 00:39:17,124
つまり

575
00:39:17,458 --> 00:39:22,663
イメージの特定を始める前に
初期化されることがなく

576
00:39:23,096 --> 00:39:26,433
エレベータや列車など
地面が動く時に

577
00:39:26,533 --> 00:39:30,470
ワールドトラッキングが
失敗した場合でも機能します

578
00:39:31,338 --> 00:39:33,040
こういう場合

579
00:39:33,140 --> 00:39:36,610
ARKitは全フレームの
位置と向きを

580
00:39:36,710 --> 00:39:38,345
１秒60フレームで推定

581
00:39:39,246 --> 00:39:42,816
これは４行のシンプルな
コードで実行できます

582
00:39:43,884 --> 00:39:49,223
ARImageTrackingConfiguration
を作成し

583
00:39:49,323 --> 00:39:52,826
追跡したいイメージを
指定します

584
00:39:52,993 --> 00:39:57,464
ここではネコとイヌ
鳥の写真を指定します

585
00:39:59,433 --> 00:40:02,770
イメージを
いくつ追跡したいかを設定

586
00:39:59,433 --> 00:40:02,770
イメージを
いくつ追跡したいかを設定

587
00:40:02,903 --> 00:40:05,239
ここでは２つにします

588
00:40:05,973 --> 00:40:09,777
２つのイメージまでは
同時に追えますが

589
00:40:09,877 --> 00:40:11,945
３つ同時は不可です

590
00:40:12,379 --> 00:40:17,217
２つのイメージを追跡していて
３つ目が出てきても

591
00:40:17,518 --> 00:40:22,156
追跡はされませんが
検出情報が更新されます

592
00:40:23,056 --> 00:40:26,427
この構成でセッションを
起動します

593
00:40:27,594 --> 00:40:31,565
ワールドトラッキングを使用しても
同じことが可能です

594
00:40:31,665 --> 00:40:34,368
この２行を書き換えます

595
00:40:35,068 --> 00:40:37,871
イメージ検出と
トラッキングの違いは

596
00:40:37,971 --> 00:40:40,274
トラッキングできるイメージの
最大数です

597
00:40:40,574 --> 00:40:45,646
イメージ検出をする
アプリケーションがある場合

598
00:40:45,746 --> 00:40:50,117
これを追加し リコンパイルすれば
トラッキングも可能になります

599
00:40:50,818 --> 00:40:53,520
どれだけ簡単か見せるため

600
00:40:53,620 --> 00:40:55,456
Xcodeの実演をします

601
00:40:56,723 --> 00:41:01,628
(拍手)

602
00:40:56,723 --> 00:41:01,628
(拍手)

603
00:41:02,696 --> 00:41:05,065
AR2に行ける？　よし

604
00:41:05,365 --> 00:41:08,102
この実演で作成するのは

605
00:41:08,202 --> 00:41:10,270
AR写真フレーム

606
00:41:10,370 --> 00:41:13,440
うちのネコの写真を
用意しました

607
00:41:13,674 --> 00:41:15,742
Xcodeを使って作ります

608
00:41:16,477 --> 00:41:18,779
まず作成したのは

609
00:41:18,879 --> 00:41:22,816
iOSのアプリケーションの
テンプレート

610
00:41:22,916 --> 00:41:25,786
今のところ ほぼ空白です

611
00:41:26,620 --> 00:41:30,157
次は添付したいイメージを指定

612
00:41:30,424 --> 00:41:34,728
うちのネコ デイジーの写真を
インポートしました

613
00:41:36,897 --> 00:41:38,065
私のネコです

614
00:41:39,900 --> 00:41:42,202
名前の指定が必要です

615
00:41:42,402 --> 00:41:44,805
デイジーと名前をつけます

616
00:41:44,905 --> 00:41:47,941
次は 現実世界での
写真の幅を指定

617
00:41:48,042 --> 00:41:51,145
私の写真フレームの幅です

618
00:41:52,146 --> 00:41:55,749
ネコの動画もロードしました

619
00:41:56,250 --> 00:41:57,818
全部 まとめます

620
00:41:58,585 --> 00:42:02,790
まずは構成を作成します
構成のタイプは

621
00:41:58,585 --> 00:42:02,790
まずは構成を作成します
構成のタイプは

622
00:42:02,956 --> 00:42:06,793
ARImageTrackingConfiguration

623
00:42:07,427 --> 00:42:10,330
アセットカタログから
追跡するイメージをロード

624
00:42:10,430 --> 00:42:13,567
グループ名“Photos”を使います

625
00:42:13,667 --> 00:42:16,070
含まれるイメージは１つだけ

626
00:42:16,170 --> 00:42:18,138
ネコのデイジーの写真です

627
00:42:18,839 --> 00:42:21,175
次に行う設定は

628
00:42:21,275 --> 00:42:24,845
trackingImagesプロパティに
追跡するイメージを構成

629
00:42:24,945 --> 00:42:25,812
ここです

630
00:42:26,080 --> 00:42:29,483
追跡するイメージの
最大数を指定します

631
00:42:30,350 --> 00:42:33,787
この時点でアプリケーションは
ARSessionを開始

632
00:42:33,887 --> 00:42:36,823
イメージ検出後に
イメージアンカーが得られます

633
00:42:36,990 --> 00:42:38,458
コンテンツを追加します

634
00:42:38,892 --> 00:42:41,228
動画をロードします

635
00:42:42,062 --> 00:42:48,068
リソースパネルから
AVプレーヤーをロードします

636
00:42:48,969 --> 00:42:52,439
それを実際のイメージ上に
追加します

637
00:42:55,008 --> 00:42:58,779
アンカーが
イメージアンカーであるか確認

638
00:42:59,279 --> 00:43:05,285
シーンのイメージと物理的寸法が
同一のSCNPlaneを作成します

639
00:42:59,279 --> 00:43:05,285
シーンのイメージと物理的寸法が
同一のSCNPlaneを作成します

640
00:43:06,186 --> 00:43:09,757
ビデオプレーヤーを
Planeのテクスチャに指定

641
00:43:09,857 --> 00:43:12,159
ビデオプレーヤーの
再生を始めます

642
00:43:13,160 --> 00:43:17,397
geometryからSCNNodeを作り
逆方向に回転させて

643
00:43:17,498 --> 00:43:21,368
アンカーの座標に合わせます

644
00:43:22,302 --> 00:43:24,171
以上で作動します

645
00:43:24,271 --> 00:43:26,707
ライブで見てみましょう

646
00:43:27,508 --> 00:43:28,709
それでは―

647
00:43:30,043 --> 00:43:33,647
写真のフレームを
カメラの前に出すと

648
00:43:34,014 --> 00:43:37,151
ビデオが再生
ネコの反応が見られます

649
00:43:37,251 --> 00:43:43,257
(拍手)

650
00:43:44,291 --> 00:43:47,261
ARKitはリアルタイムで
位置を推定するため

651
00:43:47,361 --> 00:43:49,296
デバイスを自由に動かせます

652
00:43:49,396 --> 00:43:51,064
オブジェクトも動かせます

653
00:43:51,165 --> 00:43:54,601
フレームごとに更新されます

654
00:43:55,636 --> 00:43:56,970
ネコが逃げました

655
00:43:57,971 --> 00:44:00,841
実演は以上
スライドに戻ります

656
00:43:57,971 --> 00:44:00,841
実演は以上
スライドに戻ります

657
00:44:00,941 --> 00:44:06,947
(拍手)

658
00:44:08,248 --> 00:44:12,953
ARKitでイメージトラッキングを
使うのはすごく簡単です

659
00:44:13,053 --> 00:44:16,190
ネコの動画を
撮るよりもラクです

660
00:44:16,323 --> 00:44:17,858
(笑い声)

661
00:44:17,958 --> 00:44:22,129
イメージトラッキングは
2Dのオブジェクトに向いてます

662
00:44:22,229 --> 00:44:25,198
扱う対象は
2D以外にもあります

663
00:44:25,299 --> 00:44:28,735
次はオブジェクト検出について
説明します

664
00:44:33,607 --> 00:44:39,012
オブジェクト検出は
3Dオブジェクトの検出に使います

665
00:44:39,780 --> 00:44:44,685
イメージ検出と同様
静止したオブジェクトが対象であり

666
00:44:44,785 --> 00:44:47,688
オブジェクトは動いていては
いけません

667
00:44:48,222 --> 00:44:51,925
オブジェクトの好例は
美術品の展示品

668
00:44:52,025 --> 00:44:54,395
玩具や家庭用品などです

669
00:44:56,430 --> 00:44:57,831
イメージ検出と同様

670
00:44:57,931 --> 00:45:02,236
iOSのアプリケーションで
まずスキャンします

671
00:44:57,931 --> 00:45:02,236
iOSのアプリケーションで
まずスキャンします

672
00:45:03,303 --> 00:45:07,941
ソースコードを提供している
フル機能のiOSアプリケーションで

673
00:45:08,042 --> 00:45:10,911
3Dオブジェクトを
スキャンできます

674
00:45:11,245 --> 00:45:14,615
オブジェクトには
いくつか要件があります

675
00:45:14,948 --> 00:45:18,452
質感があり硬質で
反射しないこと

676
00:45:18,552 --> 00:45:22,022
テーブルに載るくらいの
サイズが必要です

677
00:45:23,657 --> 00:45:27,027
ARKitはオブジェクトの
位置と向きを

678
00:45:27,127 --> 00:45:29,563
6DoFに基づき推定します

679
00:45:31,665 --> 00:45:35,135
全機能はワールドトラッキングに
統合されています

680
00:45:35,235 --> 00:45:39,807
オブジェクト検出開始には
プロパティを１つ設定するだけです

681
00:45:40,574 --> 00:45:42,810
手順を見てみましょう

682
00:45:43,977 --> 00:45:45,679
ARReferenceImageを

683
00:45:45,779 --> 00:45:48,549
ファイルかXcodeの
アセットカタログからロードします

684
00:45:49,316 --> 00:45:51,919
ARReferenceObjectについて
すぐに説明します

685
00:45:52,319 --> 00:45:56,590
ARReferenceObjectの
ロードが終わったら

686
00:45:58,025 --> 00:46:01,762
ARWorldTrackingConfiguration
の設定をします

687
00:45:58,025 --> 00:46:01,762
ARWorldTrackingConfiguration
の設定をします

688
00:46:01,862 --> 00:46:04,598
detectionObjectsの
プロパティを指定

689
00:46:05,599 --> 00:46:10,738
構成の設定が済んだら
セッションを開始します

690
00:46:11,271 --> 00:46:13,040
イメージ検出と同様

691
00:46:13,140 --> 00:46:16,243
ARSessionの作動中は
更新ごとにARFrameが得られます

692
00:46:16,510 --> 00:46:17,978
この場合―

693
00:46:18,479 --> 00:46:21,014
シーンにオブジェクトが
検出されると

694
00:46:21,115 --> 00:46:26,720
ARFrameの一部として
ARObjectAnchorが得られます

695
00:46:28,722 --> 00:46:32,860
このようなARオブジェクトは
ARAnchorのサブクラスです

696
00:46:32,960 --> 00:46:37,865
従って 位置･向き･6DoFを表す
transformを持ちます

697
00:46:38,098 --> 00:46:42,336
ARReferenceObjectへの
参照点が提示されるため

698
00:46:42,436 --> 00:46:45,205
どのオブジェクトを
検出したかが分かります

699
00:46:46,173 --> 00:46:48,108
この機能は

700
00:46:49,209 --> 00:46:52,579
３行の簡単なコードで
実装できます

701
00:46:52,913 --> 00:46:56,483
ARWorldTrackingConfiguration
の構成を作成

702
00:46:56,617 --> 00:47:00,120
検出したいオブジェクトを
指定します

703
00:46:56,617 --> 00:47:00,120
検出したいオブジェクトを
指定します

704
00:47:00,420 --> 00:47:04,425
ここではARの
博物館アプリケーションを作り

705
00:47:04,525 --> 00:47:07,928
古代の胸像や土器を検出します

706
00:47:08,829 --> 00:47:11,531
これを使い
セッションを起動します

707
00:47:12,299 --> 00:47:17,171
簡単な博物館アプリケーションを
オフィスで作ってきました

708
00:47:17,271 --> 00:47:18,739
見てみましょう

709
00:47:18,906 --> 00:47:24,378
iOSアプリケーションで
この胸像が見えてくると

710
00:47:24,478 --> 00:47:28,849
6DoFの姿勢が得られ
シーンが提示されます

711
00:47:28,949 --> 00:47:32,453
像の上には
簡単な説明が浮かびます

712
00:47:32,753 --> 00:47:34,655
今回 説明に加えたのは

713
00:47:34,988 --> 00:47:38,492
エジプト王妃 ネフェルティティの
名前と生年月日

714
00:47:38,592 --> 00:47:42,696
レンダリングエンジンで使える
あらゆるコンテンツを追加できます

715
00:47:43,797 --> 00:47:47,768
アプリケーションの構築には
オブジェクトのスキャンが必要です

716
00:47:47,935 --> 00:47:50,170
スキャンについて説明します

717
00:47:51,739 --> 00:47:56,210
オブジェクトのスキャンでは
蓄積シーン情報を抽出します

718
00:47:57,010 --> 00:48:00,247
平面の推定と
深い関係があります

719
00:47:57,010 --> 00:48:00,247
平面の推定と
深い関係があります

720
00:48:00,347 --> 00:48:05,519
蓄積シーン情報は水平面と
垂直面の推定に使われます

721
00:48:05,953 --> 00:48:08,122
今回 私たちは

722
00:48:08,222 --> 00:48:10,724
この情報を用いて

723
00:48:11,058 --> 00:48:13,861
3Dオブジェクトの
情報を集めました

724
00:48:14,995 --> 00:48:18,699
オブジェクトを探す領域を
指定するために

725
00:48:18,799 --> 00:48:22,403
transformとextentとcenterを
指定します

726
00:48:22,636 --> 00:48:25,205
これはオブジェクトを囲む
境界ボックスであり

727
00:48:25,305 --> 00:48:28,442
シーン内の位置を定義します

728
00:48:29,777 --> 00:48:33,781
抽出されたオブジェクトは
アセットカタログに対応

729
00:48:33,881 --> 00:48:36,450
これにより手軽に―

730
00:48:37,151 --> 00:48:41,488
新しいアプリケーションに組み込め
何度でも使えるようになります

731
00:48:42,456 --> 00:48:47,061
ARObjectScanningConfiguration
をスキャン用に追加しました

732
00:48:48,028 --> 00:48:51,865
自らスキャン用アプリケーションを
実装する必要はありません

733
00:48:51,965 --> 00:48:56,136
フル機能のアプリケーションの
サンプルコードが使えます

734
00:48:56,236 --> 00:48:59,073
“Scanning and detecting
3D objects”です

735
00:49:00,007 --> 00:49:02,609
使い方を見てみましょう

736
00:49:02,943 --> 00:49:06,280
興味のあるオブジェクトを
境界ボックスで囲みます

737
00:49:06,380 --> 00:49:08,515
ネフェルティティ像です

738
00:49:09,183 --> 00:49:12,786
オブジェクトを
ぴったり囲む必要はなく

739
00:49:12,886 --> 00:49:18,258
重要な特徴点が
内側に収まっていれば大丈夫です

740
00:49:19,059 --> 00:49:21,295
境界ボックスができたら

741
00:49:21,695 --> 00:49:24,731
“Scan”を押します

742
00:49:24,832 --> 00:49:26,934
スキャンが開始します

743
00:49:27,034 --> 00:49:30,637
進行状態を画面上で
見ることができます

744
00:49:30,738 --> 00:49:33,540
オブジェクトの
スキャンの状況が

745
00:49:33,640 --> 00:49:35,142
空間的に表示されます

746
00:49:35,843 --> 00:49:39,179
すべての角度からの
スキャンは不要です

747
00:49:39,413 --> 00:49:44,151
たとえば 博物館で
像が壁に向いていて

748
00:49:44,251 --> 00:49:49,022
特定の角度から
検出できないのであれば

749
00:49:49,123 --> 00:49:51,391
その面のスキャンは無用です

750
00:49:53,260 --> 00:49:56,030
満足のいく
スキャンができたら

751
00:49:56,864 --> 00:49:58,031
範囲の中心を

752
00:49:59,433 --> 00:50:01,401
調整できます

753
00:49:59,433 --> 00:50:01,401
調整できます

754
00:50:01,568 --> 00:50:03,771
オブジェクトの起点に
相当します

755
00:50:04,204 --> 00:50:08,809
中心はオブジェクトの
範囲内にある必要があります

756
00:50:09,843 --> 00:50:14,681
最後にアプリケーションで
検出テストを実行します

757
00:50:15,215 --> 00:50:19,520
今回の場合 さまざまな視点から
検出ができており

758
00:50:19,620 --> 00:50:20,788
いいスキャンです

759
00:50:21,789 --> 00:50:24,425
オブジェクトを
動かしてみましょう

760
00:50:24,525 --> 00:50:28,529
違う場所で
テクスチャや照明が変わっても

761
00:50:29,363 --> 00:50:32,599
検出できるかどうか試すのです

762
00:50:34,301 --> 00:50:39,006
スキャンのあとは
ARReferenceObjectが得られます

763
00:50:39,406 --> 00:50:41,708
先ほど図で見せたものです

764
00:50:42,109 --> 00:50:44,344
ARオブジェクトファイルの
拡張タイプに

765
00:50:44,445 --> 00:50:47,381
オブジェクトを
シリアル化できます

766
00:50:47,481 --> 00:50:52,152
名前がつけられ
アセットカタログに表示されます

767
00:50:52,252 --> 00:50:55,689
スキャンに用いた
中心と範囲も含まれます

768
00:50:56,390 --> 00:50:58,892
スキャンを行った領域の

769
00:50:58,992 --> 00:51:02,863
特徴点も
すべて取得します

770
00:50:58,992 --> 00:51:02,863
特徴点も
すべて取得します

771
00:51:05,232 --> 00:51:06,900
以上がオブジェクト検出です

772
00:51:07,101 --> 00:51:10,838
オブジェクトを検出する前には
スキャンが必要です

773
00:51:10,938 --> 00:51:14,274
完全なソースコードは
現在 ダウンロード可能です

774
00:51:16,210 --> 00:51:19,713
次はフェイストラッキングについて
解説します

775
00:51:24,284 --> 00:51:26,687
昨年 iPhone Xを
リリースした際

776
00:51:26,787 --> 00:51:30,224
強力な顔検出と追跡を
ARKitに追加しました

777
00:51:30,591 --> 00:51:34,395
ARKitは
１秒あたり60フレームで

778
00:51:34,495 --> 00:51:37,297
顔の位置と
向きを推定します

779
00:51:37,664 --> 00:51:40,200
このポーズを利用して

780
00:51:40,300 --> 00:51:43,771
ユーザの顔に
仮面や帽子を被せたり

781
00:51:43,871 --> 00:51:46,373
顔全体のテクスチャを
取り替えたりできます

782
00:51:47,641 --> 00:51:52,446
ARFaceGeometryの形式の
三角形メッシュもあります

783
00:51:54,781 --> 00:51:58,018
顔面メッシュの
レンダリングに必要な情報が

784
00:51:58,152 --> 00:52:00,220
ARFaceGeometryに
すべて含まれています

785
00:51:58,152 --> 00:52:00,220
ARFaceGeometryに
すべて含まれています

786
00:52:00,387 --> 00:52:02,456
表示形式には

787
00:52:02,823 --> 00:52:07,061
頂点･三角形･検出座標が
用いられます

788
00:52:08,729 --> 00:52:11,765
主要なアンカーのタイプは
ARFaceAnchorです

789
00:52:11,865 --> 00:52:15,602
フェイストラッキングに
必要な情報がすべて含まれます

790
00:52:16,970 --> 00:52:20,040
幾何学イメージをリアルに
レンダリングするために

791
00:52:20,274 --> 00:52:22,910
指向性照明推定を
追加しました

792
00:52:23,577 --> 00:52:26,213
ARKitは照明を
ライトプローブとして使い

793
00:52:26,313 --> 00:52:31,118
ARDirectionalLightEstimateを推定

794
00:52:31,218 --> 00:52:35,522
照明の強度･方向･色温度が
推定に含まれます

795
00:52:36,490 --> 00:52:38,192
この推定により

796
00:52:38,292 --> 00:52:42,229
アプリケーションの見ばえを
十分 よくできます

797
00:52:42,329 --> 00:52:44,932
アプリケーションを
凝ったものにしたい場合は

798
00:52:45,332 --> 00:52:49,203
２次の球面調和関数の係数も
提供しています

799
00:52:49,303 --> 00:52:52,339
シーン全体の
照明の状態を収集でき

800
00:52:52,439 --> 00:52:56,076
コンテンツの見ばえを
さらに よくできます

801
00:52:57,778 --> 00:53:00,848
ARKitはリアルタイムで
表情の追跡もできます

802
00:52:57,778 --> 00:53:00,848
ARKitはリアルタイムで
表情の追跡もできます

803
00:53:01,014 --> 00:53:07,021
ブレンドシェイプと呼ばれる
50種類以上の表情があります

804
00:53:08,055 --> 00:53:10,958
ブレンドシェイプは
０と１の間の値からなります

805
00:53:11,058 --> 00:53:14,127
１はアクティブな状態を意味し
０はその逆です

806
00:53:14,228 --> 00:53:16,697
たとえば あごの開放係数は

807
00:53:16,897 --> 00:53:21,635
口を開けると値が１
閉じると０に近づきます

808
00:53:22,636 --> 00:53:25,873
バーチャルなキャラクターを
動かす時に使えます

809
00:53:26,040 --> 00:53:30,344
この例では
口と左右の目の開閉を用いて

810
00:53:30,444 --> 00:53:33,146
シンプルな顔のキャラクターを
動かしています

811
00:53:34,081 --> 00:53:35,749
もっと うまくできます

812
00:53:36,150 --> 00:53:38,452
実際 アニ文字を作る際には

813
00:53:38,552 --> 00:53:40,888
ブレンドシェイプを
もっと多く使っていました

814
00:53:40,988 --> 00:53:45,125
これら動く青いバーを使い
頭部の姿勢を取得

815
00:53:45,225 --> 00:53:48,495
私の表情を
パンダの顔にマッピングします

816
00:53:49,596 --> 00:53:54,501
キャラクターを動かすのに
必要なものをすべて提供しています

817
00:53:54,601 --> 00:53:56,537
アニ文字と同様です

818
00:53:58,872 --> 00:54:00,474
ありがとうございます

819
00:53:58,872 --> 00:54:00,474
ありがとうございます

820
00:54:00,574 --> 00:54:04,611
(拍手)

821
00:54:04,812 --> 00:54:08,182
ARKit 2のフェイストラッキングの
新機能を見てみます

822
00:54:08,582 --> 00:54:10,284
視線追跡の追加

823
00:54:10,384 --> 00:54:14,621
左右の目を6DoFで追跡します

824
00:54:16,990 --> 00:54:18,125
ご覧のとおり

825
00:54:18,225 --> 00:54:21,295
(拍手)

826
00:54:21,395 --> 00:54:24,531
これらのプロパティは
ARFaceAnchorの一部です

827
00:54:24,631 --> 00:54:29,870
２つの視線が
交差する注視点も含まれます

828
00:54:30,170 --> 00:54:34,241
この情報は
キャラクターを動かすことや

829
00:54:34,341 --> 00:54:37,244
アプリケーションへの
他のインプットにも使えます

830
00:54:37,678 --> 00:54:38,846
他にもあります

831
00:54:39,546 --> 00:54:41,281
舌のサポートを追加

832
00:54:42,015 --> 00:54:44,184
新しいブレンドシェイプで
表されます

833
00:54:44,485 --> 00:54:46,987
舌が出ていると値は１

834
00:54:47,087 --> 00:54:49,056
出ていないと０です

835
00:54:49,389 --> 00:54:52,726
これもキャラクターの動きや

836
00:54:52,826 --> 00:54:56,730
アプリケーションへの
インプットに使えます

837
00:55:00,067 --> 00:55:01,535
ありがとうございます

838
00:55:01,635 --> 00:55:03,470
(拍手)

839
00:55:03,570 --> 00:55:06,440
何度も舌を出す自分を
見ながら

840
00:55:06,540 --> 00:55:08,375
要約をします

841
00:55:08,842 --> 00:55:09,877
それでは

842
00:55:10,277 --> 00:55:12,946
ARKit 2の新機能を
確認しましょう

843
00:55:13,647 --> 00:55:17,951
マップの保存とロードは
永続性を提供する新機能で

844
00:55:18,052 --> 00:55:20,854
複数ユーザの
コラボを可能にします

845
00:55:21,688 --> 00:55:26,126
ワールドトラッキングの改良で
平面検出が正確かつ迅速になり

846
00:55:26,226 --> 00:55:28,595
新しいビデオフォーマットも
使えます

847
00:55:29,396 --> 00:55:31,465
環境テクスチャリングは

848
00:55:31,565 --> 00:55:36,437
コンテンツを まるで本当に
シーンに存在するように見せます

849
00:55:36,537 --> 00:55:41,041
シーンのテクスチャを収集し
オブジェクトの細部に利用します

850
00:55:41,775 --> 00:55:43,811
イメージトラッキングは

851
00:55:46,280 --> 00:55:51,318
2Dオブジェクトを
イメージとして追跡できます

852
00:55:51,418 --> 00:55:54,154
ARKitは3Dオブジェクトの
検出も可能です

853
00:55:54,488 --> 00:55:58,025
フェイストラッキングでは
視線や舌にも対応

854
00:55:59,760 --> 00:56:04,665
以上はすべてARKitの
基本機能として利用可能です

855
00:55:59,760 --> 00:56:04,665
以上はすべてARKitの
基本機能として利用可能です

856
00:56:06,133 --> 00:56:09,069
iOS 12のARKitには
５つの異なる構成があり

857
00:56:09,169 --> 00:56:10,504
２つは新登場です

858
00:56:10,604 --> 00:56:14,675
単独でのイメージトラッキング用の
ARImageTrackingConfiguration

859
00:56:14,775 --> 00:56:17,211
そして
ARObjectScanningConfiguration

860
00:56:17,945 --> 00:56:23,150
ARSessionと連動する
一連の補助タイプには

861
00:56:23,384 --> 00:56:25,919
ARFrameや
ARCameraがあります

862
00:56:26,620 --> 00:56:27,821
これには新たに２つが追加

863
00:56:27,921 --> 00:56:30,190
オブジェクト検出用の
ARReferenceObject

864
00:56:30,290 --> 00:56:33,794
永続性を有し
複数ユーザで使えるARWorldMap

865
00:56:34,795 --> 00:56:38,365
そして現実世界での
位置を表すARAnchor

866
00:56:38,465 --> 00:56:40,834
追加が２つ

867
00:56:40,934 --> 00:56:42,369
ARObjectAnchorと

868
00:56:42,469 --> 00:56:44,371
AREnvironmentProbeAnchorです

869
00:56:45,606 --> 00:56:49,810
iOSで現在利用可能な
ARKitの機能を使い

870
00:56:49,910 --> 00:56:52,579
皆さんが何を作るか楽しみです

871
00:56:53,981 --> 00:56:59,987
(拍手)

872
00:57:00,788 --> 00:57:03,857
別のセッションでは
AR Quick Lookを統合し

873
00:57:03,957 --> 00:57:07,861
コンテンツをカッコよく
見せる方法を紹介します

874
00:57:08,862 --> 00:57:10,898
以上です
ありがとうございます

875
00:57:10,998 --> 00:57:12,399
WWDCを楽しんでください

876
00:57:12,499 --> 00:57:13,700
(拍手)